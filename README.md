# Zero to Hero Transformers and Attention üöÄ

This is a curated collection of resources from key people in AI for understanding and building attention mechanisms and Transformer models. Organized by papers, books, courses, blogs, and interesting research (i.e. other fun and interesting areas). These resources guide a [non-credit undergraduate course](https://github.com/jlaitue/attention_is_all_indeed/blob/main/ZeroHeroCourse.md) at NYUAD I created for Spring 2025. Getting those 10,000 hours down to become an iniated knowledge machine. I envision that future me will thank me (myself and I) and other students for getting good at this while complementing my PhD research for the future of AI in healthcare and beyond!

"Stand on the shoulders of giants"

---

## üìú Papers

1. **[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)** - *Bahdanau et al., 2014*  
   Introduces the concept of attention in neural machine translation, allowing models to focus on different parts of the input sequence dynamically.

2. **[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025)** - *Luong et al., 2014*
   Another paper on attention from Stanford.
   
3. **[Attention Is All You Need](https://arxiv.org/abs/1706.03762)** - *Vaswani et al., 2017*  
   The seminal paper introducing the Transformer model, replacing recurrence with self-attention and enabling parallelization in deep learning.

---

## üìö Books

1. **[The Art of Transformer Programming](https://yanivle.github.io/taotp.pdf)** - *Yaniv Leviathan*  
   A deep dive into best practices for implementing transformer models efficiently.

2. **[Build Large Language Models from Scratch](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167?crid=228R4JI0P0QFR&dib=eyJ2IjoiMSJ9.XvZyIer9iV133BWXqNiVt_OOJXZheO54dvZtQly8MC25PNYZrN3OWsGLjbg3I0G9hI3LkjwhsORxvHIob3nvCZFgdSSQEFe07VkehijGxT03n4Amdw7lnXxnsOUuWXeglfHnewCcV3DjL9zWHELfh5DG1ZErzFym3S6ZxSuFzNvoPkaq0uDlD_CKwqHdC0KM_RdvIqF0_2RudgvzRli0V155KkusHRck3pG7ybp5VyqKDC_GgL_MEywLwLhFgX6kOCgV6Rq90eTgSHFd6ac8krpIYjsHWe6H3IXbfKGvMXc.473O1-iUZC0z2hdx8L5Z5ZTNxtNV9gNPw_mE7QZ5Y90&dib_tag=se&keywords=raschka&qid=1730250834&sprefix=raschk,aps,162&sr=8-1&linkCode=sl1&tag=rasbt03-20&linkId=84ee23afbd12067e4098443718842dac&language=en_US&ref_=as_li_ss_tl)** - *Sebastian Raschka*  
**[Github repo accompanying the book](https://github.com/rasbt/LLMs-from-scratch)**
   A short course that guides learners through building large language models from scratch, with practical examples.

3. **[How to Scale Your Model](https://jax-ml.github.io/scaling-book/index)** - *Jax Deepmind Team*  
   Training LLMs often feels like alchemy, but understanding and optimizing the performance of your models doesn't have to. This book aims to demystify the science of scaling language models on TPUs

---

## üéì Courses/Slides/Talks/Repos

1. **[Course: Attention in Transformers | Concepts and Code in PyTorch](https://www.deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch/?utm_campaign=joshstarmer-launch&utm_medium=social&utm_source=x)** - *Josh Starmer DeepLearning.AI*  
   A practical introduction to attention mechanisms and transformer models, with coding exercises in PyTorch. From the great Josh Starmer, CEO of StatQuest. BAM!

2. **[Talk: Transformers at M2L Summer School 2024](https://www.youtube.com/watch?v=bMXqnLiVgLk)** - *Lucas Beyer OpenAI* 
   Talk from Lucas Beyer transformer models for the M2L Summer School 2024.

3. **[Talk: Computer Vision in the Age of LLMs at ML in PL 2024](https://www.youtube.com/watch?v=kxO6ARgI_SU)** - *Lucas Beyer OpenAI* 
   Discusses how computer vision has changed with the integration of language and the advent of LLMs, with focus on the recent works of their group.

4. **[Course: Zero to Hero Series](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)**  - *Andrej Karpathy* 
   A hands-on series covering neural networks, GPT-style transformers from one of the OGs of ImageNet, OpenAI, etc. This guy doesn't need me to introduce him ha.

5. **[A Practical Guide for Medical Large Language Models](https://github.com/AI-in-Health/MedLLMsPracticalGuide)**
   Actively updated list of practical guide resources for Medical Large Language Models (Medical LLMs) based on a survey from the authors of the repo

---

## üìù Blogs

1. **[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)** - *Jay Alammar*  
   A visually intuitive explanation of the Transformer model, attention mechanism and its key components.

---

## üîç Interesting Research

1. **[Byte-Latent Transformer: Patches Scale Better Than Tokens](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/)** - Meta AI  
   Investigates the effectiveness of using patches instead of tokens in transformers.

2. **[Hierarchical Transformers for Long Document Classification](https://arxiv.org/pdf/2201.09792)**  
   Proposes a hierarchical approach to apply transformers to lengthy documents.

3. **[Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2201.03545)**  
   A study on improving transformers for tasks beyond traditional left-to-right text generation.

4. **[Diffusion-LM Improves Compositional Generalization](https://arxiv.org/abs/2211.17192)**  
   Explores diffusion models for improving generalization in language models.

5. **[Looking Back at Speculative Decoding](https://research.google/blog/looking-back-at-speculative-decoding/)** - Google Research  
   A retrospective on techniques for improving decoding efficiency in transformer models.

---

üöÄ *Have fun learning!*  
