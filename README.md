# Zero to Hero Transformers and Attention üöÄ

This is a curated collection of resources from key people in AI for understanding and building attention mechanisms and Transformer models. Organized by papers, books, courses, blogs, and interesting research (other fun and interesting areas). Perhaps in an update of the repo I'll list the working plan I'm following. I am currently a loser in these topics so I wanted to get those 10,000 hours down to become an iniated knowledge machine. I envision that future me will thank me (myself and I) for getting good at this as it also complements my PhD research for the future of AI in healthcare and beyond!

"Stand on the shoulders of giants"

---

## üìú Papers

1. **[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)** - *Bahdanau et al., 2014*  
   Introduces the concept of attention in neural machine translation, allowing models to focus on different parts of the input sequence dynamically.

2. **[Attention Is All You Need](https://arxiv.org/abs/1706.03762)** - *Vaswani et al., 2017*  
   The seminal paper introducing the Transformer model, replacing recurrence with self-attention and enabling parallelization in deep learning.

---

## üìö Books

1. **[The Art of Transformer Programming](https://yanivle.github.io/taotp.pdf)** - *Yaniv Leviathan*  
   A deep dive into best practices for implementing transformer models efficiently.

2. **[Build Large Language Models from Scratch](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167?crid=228R4JI0P0QFR&dib=eyJ2IjoiMSJ9.XvZyIer9iV133BWXqNiVt_OOJXZheO54dvZtQly8MC25PNYZrN3OWsGLjbg3I0G9hI3LkjwhsORxvHIob3nvCZFgdSSQEFe07VkehijGxT03n4Amdw7lnXxnsOUuWXeglfHnewCcV3DjL9zWHELfh5DG1ZErzFym3S6ZxSuFzNvoPkaq0uDlD_CKwqHdC0KM_RdvIqF0_2RudgvzRli0V155KkusHRck3pG7ybp5VyqKDC_GgL_MEywLwLhFgX6kOCgV6Rq90eTgSHFd6ac8krpIYjsHWe6H3IXbfKGvMXc.473O1-iUZC0z2hdx8L5Z5ZTNxtNV9gNPw_mE7QZ5Y90&dib_tag=se&keywords=raschka&qid=1730250834&sprefix=raschk,aps,162&sr=8-1&linkCode=sl1&tag=rasbt03-20&linkId=84ee23afbd12067e4098443718842dac&language=en_US&ref_=as_li_ss_tl)** - *Sebastian Raschka*  
**[Github repo accompanying the book](https://github.com/rasbt/LLMs-from-scratch)**
   A short course that guides learners through building large language models from scratch, with practical examples.

---

## üéì Courses/Slides

1. **[Attention in Transformers: Concepts and Code in PyTorch](https://www.deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch/?utm_campaign=joshstarmer-launch&utm_medium=social&utm_source=x)** - *Josh Starmer DeepLearning.AI*  
   A practical introduction to attention mechanisms and transformer models, with coding exercises in PyTorch. From the great Josh Starmer, CEO of StatQuest. BAM!

2. **[Lucas Beyer: M2L Summer School 2024](https://www.youtube.com/watch?v=bMXqnLiVgLk)**
   Talk from Lucas Beyer transformer models for the M2L Summer School 2024.

3. **[Andrej Karpathy: Zero to Hero Series](https://karpathy.ai/zero-to-hero/](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ )**  
   A hands-on series covering neural networks, GPT-style transformers from one of the OGs of ImageNet, OpenAI, etc. This guy doesn't need me to introduce him ha.

---

## üìù Blogs

1. **[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)** - *Jay Alammar*  
   A visually intuitive explanation of the Transformer model, attention mechanism and its key components.

---

## üîç Interesting Research

1. **[Byte-Latent Transformer: Patches Scale Better Than Tokens](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/)** - Meta AI  
   Investigates the effectiveness of using patches instead of tokens in transformers.

2. **[Hierarchical Transformers for Long Document Classification](https://arxiv.org/pdf/2201.09792)**  
   Proposes a hierarchical approach to apply transformers to lengthy documents.

3. **[Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2201.03545)**  
   A study on improving transformers for tasks beyond traditional left-to-right text generation.

4. **[Diffusion-LM Improves Compositional Generalization](https://arxiv.org/abs/2211.17192)**  
   Explores diffusion models for improving generalization in language models.

5. **[Looking Back at Speculative Decoding](https://research.google/blog/looking-back-at-speculative-decoding/)** - Google Research  
   A retrospective on techniques for improving decoding efficiency in transformer models.

---

üöÄ *Have fun learning!*  
