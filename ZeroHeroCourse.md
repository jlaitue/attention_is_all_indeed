# Overview

- **Schedule:** Weekly on Tuesday, 11.15 am - 12.15 pm
- **Duration:** March 4th, 2025 – May 6th, 2025 (10 sessions, Ramadan break is considered)
- **Place:** C2-339 (Pending confirmation)
- **Format:** Interactive lectures with discussions and code demonstrations
- **Evaluation:** None – Focus on conceptual understanding and practical insights
- **Contact:** [PhD candidate Julián Lechuga](ljl5178@nyu.edu)

# Topics

## **1. Attention Mechanism**

### **Session 1 (**March 4th**)**

Introduction to Attention Mechanisms

- Why attention? (Limitations of traditional models)
- Core concept: Query, Key, Value
- Key insights behind attention: Self-attention vs. Cross-attention

### **Session 2 (**March 11th**)**

Computing Attention

- Mathematical formulation
- Scaled Dot-Product Attention
- Multi-head attention

## **2. What is a Transformer?**

### **Session 3 (**March 18th**)**

Transformer Architecture

- How transformers evolved from attention
- Encoder-Decoder structure
- Overview of training objectives

### **Session 4 (**March 25th**)**

Key Components of Transformers

- Positional Encoding
- Layer normalization and residual connections
- Feedforward networks

### **Session 5 (**April 8th**)**

Transformer Pipeline

- Data flow in training and inference
- Computational efficiency and parallelization
- Transformer variations (e.g., BERT, GPT)

## **3. Foundation Models**

### **Session 6 (**April 15th**)**

What Are Foundation Models?

- Definition and characteristics
- Pretraining vs. Fine-tuning
- Transfer learning with large models

### **Session 7 (**April 22nd**)**

Training and Scaling Laws

- How foundation models are trained at scale
- Computational challenges
- Efficiency optimizations

### **Session 8 (**April 29th**)**

Applications and Use Cases

- Language modeling (Chatbots, Code Generation)
- Vision transformers
- Multimodal models (e.g., CLIP, DALL·E)

## **4. Societal Impact & Future Directions**

### **Session 9 & 10 (**May 6th**)**

Ethical Considerations and Risks

- Bias in foundation models
- Fairness, safety, and interpretability
- Regulation and responsible AI

Future of AI and Next-Gen Models

- Advances beyond transformers (e.g., Mixture of Experts, Diffusion Models)
- Open research challenges
- Where to go next: Reading resources & hands-on projects

# **Learning Outcomes**

By the end of this course, students should be able to:

1. **Understand and explain** the core principles behind attention mechanisms and how they power modern deep learning models.
2. **Describe** the architecture and components of Transformer models.
3. **Analyze** how foundation models are built, trained, and scaled.
4. **Discuss** real-world applications and the role of transformers in NLP, vision, and multimodal AI.
5. **Critically evaluate** the ethical and societal impacts of foundation models.
6. **Engage with** relevant research papers and explore advanced topics in AI model development.
